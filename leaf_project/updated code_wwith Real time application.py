# -*- coding: utf-8 -*-
"""MMP_final project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15rzlSwnW1RgfDPEcW8yUacHtSWuOmB18
"""

import sys
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
import os
import cv2
import pandas as pd
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Create models directory if it doesn't exist
os.makedirs('models', exist_ok=True)

# Set random seed for reproducibility
torch.manual_seed(1)
np.random.seed(1)

# Define image transformations
# For training: include data augmentation
transform_train = transforms.Compose([
    transforms.Resize((64, 64)),  # Resize images to 64x64
    transforms.RandomHorizontalFlip(),  # Random horizontal flip
    transforms.RandomRotation(30),  # Random rotation up to 30 degrees
    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Random color jitter
    transforms.ToTensor(),  # Convert to tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats
])
# For validation/testing: only resize and normalize
transform_test = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load Leaf Dataset
data_dir = r'/content/drive/MyDrive/RGB'
# Try to load the dataset (if available)
try:
    full_dataset = datasets.ImageFolder(root=data_dir, transform=transform_train)
    print(f"Dataset loaded with {len(full_dataset)} images and {len(full_dataset.classes)} classes")

    # If using UCI Leaf dataset or Leafsnap, you would load it differently
    # This is a placeholder for that specific dataset loading code

    # Split the dataset into train, validation, and test sets
    train_size = int(0.7 * len(full_dataset))
    val_size = int(0.15 * len(full_dataset))
    test_size = len(full_dataset) - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size]
    )

    # Apply appropriate transforms to each split
    train_dataset.dataset.transform = transform_train
    val_dataset.dataset.transform = transform_test
    test_dataset.dataset.transform = transform_test

except Exception as e:
    print(f"Error loading dataset: {e}")
    print("Please ensure the dataset is available at the specified path.")
    # The following code will continue executing but data loaders won't work
    train_dataset, val_dataset, test_dataset = None, None, None
    num_classes = 10  # Placeholder, replace with actual number of leaf classes

# Define data loaders
batch_size = 40

# Create data loaders if datasets are available
if train_dataset is not None:
    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    valid_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Get the number of classes
    num_classes = len(full_dataset.classes)
    print(f"Number of classes: {num_classes}")
    numeric_labels = [str(i + 1) for i in range(num_classes)]
    # Print class names
    print("Class names:", full_dataset.classes)

class LeafCNN(nn.Module):
    def __init__(self, num_classes):
        super(LeafCNN, self).__init__()

        self.features = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),  # Larger kernel
            nn.BatchNorm2d(16),
            nn.LeakyReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.3),

            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.4),

            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.4),
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# Initialize the model
if 'num_classes' in locals():
    model = LeafCNN(num_classes)
    print(model)

    # Select device (GPU if available, otherwise CPU)
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    model = model.to(device)

    # Define loss function and optimizer
    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Training function
    def train(model, num_epochs, train_dl, valid_dl):
        loss_hist_train = [0] * num_epochs
        accuracy_hist_train = [0] * num_epochs
        loss_hist_valid = [0] * num_epochs
        accuracy_hist_valid = [0] * num_epochs

        for epoch in range(num_epochs):
            # Training phase
            model.train()
            for x_batch, y_batch in train_dl:
                x_batch = x_batch.to(device)
                y_batch = y_batch.to(device)

                # Forward pass
                pred = model(x_batch)
                loss = loss_fn(pred, y_batch)

                # Backward pass and optimization
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

                # Record statistics
                loss_hist_train[epoch] += loss.item() * y_batch.size(0)

                # Calculate accuracy
                _, predicted = torch.max(pred, 1)
                is_correct = (predicted == y_batch).float()
                accuracy_hist_train[epoch] += is_correct.sum().cpu()

            loss_hist_train[epoch] /= len(train_dl.dataset)
            accuracy_hist_train[epoch] /= len(train_dl.dataset)

            # Validation phase
            model.eval()
            with torch.no_grad():
                for x_batch, y_batch in valid_dl:
                    x_batch = x_batch.to(device)
                    y_batch = y_batch.to(device)

                    # Forward pass
                    pred = model(x_batch)
                    loss = loss_fn(pred, y_batch)

                    # Record statistics
                    loss_hist_valid[epoch] += loss.item() * y_batch.size(0)

                    # Calculate accuracy
                    _, predicted = torch.max(pred, 1)
                    is_correct = (predicted == y_batch).float()
                    accuracy_hist_valid[epoch] += is_correct.sum().cpu()

            loss_hist_valid[epoch] /= len(valid_dl.dataset)
            accuracy_hist_valid[epoch] /= len(valid_dl.dataset)

            print(f'Epoch {epoch+1} accuracy: {accuracy_hist_train[epoch]:.4f} val_accuracy: {accuracy_hist_valid[epoch]:.4f}')

        return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid

    # Begin training if data is available
    if train_dl is not None:
        num_epochs = 40
        print("Starting training...")
        hist = train(model, num_epochs, train_dl, valid_dl)

        # Plot training history with improved layout
        x_arr = np.arange(len(hist[0])) + 1
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        # Loss plot
        ax1.plot(x_arr, hist[0], '-o', label='Train loss', markersize=4)
        ax1.plot(x_arr, hist[1], '-x', label='Validation loss', markersize=4)
        ax1.set_title("Model Loss")
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        # Accuracy plot
        ax2.plot(x_arr, hist[2], '-o', label='Train acc.', markersize=4)
        ax2.plot(x_arr, hist[3], '-x', label='Validation acc.', markersize=4)
        ax2.set_title("Model Accuracy")
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)
        plt.tight_layout()
        plt.show()


        # Evaluate on test set
        def evaluate(model, data_loader):
            model.eval()
            correct = 0
            total = 0
            all_preds = []
            all_targets = []

            with torch.no_grad():
                for inputs, targets in data_loader:
                    inputs, targets = inputs.to(device), targets.to(device)
                    outputs = model(inputs)
                    _, predicted = torch.max(outputs.data, 1)

                    total += targets.size(0)
                    correct += (predicted == targets).sum().item()

                    all_preds.extend(predicted.cpu().numpy())
                    all_targets.extend(targets.cpu().numpy())

            accuracy = correct / total
            return accuracy, all_preds, all_targets

        # Test model
        test_accuracy, predictions, targets = evaluate(model, test_dl)
        print(f"Test accuracy: {test_accuracy:.4f}")

        # Save the model
        torch.save(model.state_dict(), 'models/leaf_classifier_cnn.pth')
        print("Model saved to 'models/leaf_classifier_cnn.pth'")


        # Visualize some predictions
def visualize_predictions(model, data_loader, class_names, num_samples=10):
    model.eval()
    images, labels = next(iter(data_loader))
    images, labels = images.to(device), labels.to(device)

    with torch.no_grad():
        outputs = model(images)
        _, preds = torch.max(outputs, 1)

    # Print predictions and actual labels to console
    print("\nPredicted vs Actual Leaf Labels:")
    for i in range(min(num_samples, len(images))):
        pred_name = class_names[preds[i].item()]
        true_name = class_names[labels[i].item()]
        result = f"Sample {i+1}: Predicted = {pred_name}, Actual = {true_name}"
        print(result)

    # Visualize with labels
    plt.figure(figsize=(15, 8))
    for i in range(min(num_samples, len(images))):
        plt.subplot(2, 5, i + 1)
        img = images[i].cpu().permute(1, 2, 0)
        # Denormalize
        img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])
        img = torch.clamp(img, 0, 1)

        pred_label = class_names[preds[i].item()]
        true_label = class_names[labels[i].item()]
        color = 'green' if preds[i] == labels[i] else 'red'
        plt.imshow(img)
        plt.title(f"Pred: {pred_label}\nTrue: {true_label}", color=color)
        plt.axis('off')

    plt.tight_layout()
    plt.show()
# Create confusion matrix
# --- CNN Confusion Matrix ---
cm_cnn = confusion_matrix(targets, predictions)

# Use numeric labels only
numeric_labels = [str(i + 1) for i in range(40)]

plt.figure(figsize=(8, 6))
sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues',
            xticklabels=numeric_labels, yticklabels=numeric_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for CNN', fontsize=14)
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

secondtestimg = r'/content/drive/MyDrive/RGB'

def test_on_new_images(model, image_dir, transform, class_names):
    model.eval()
    images = []
    filenames = []

    # Load images from the directory
    for file in os.listdir(image_dir):
        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
            img_path = os.path.join(image_dir, file)
            image = Image.open(img_path).convert('RGB')
            image = transform(image).unsqueeze(0)  # Add batch dimension
            images.append(image)
            filenames.append(file)

    if not images:
        print("No valid images found in directory.")
        return

    # Concatenate into a batch
    batch = torch.cat(images).to(device)

    with torch.no_grad():
        outputs = model(batch)
        _, preds = torch.max(outputs, 1)

    # Display predictions
    for i in range(len(images)):
        pred_label = class_names[preds[i].item()]
        print(f"{filenames[i]} --> Predicted: {pred_label}")

        # Display image
        img_np = batch[i].cpu()
        img_np = img_np * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        img_np = torch.clamp(img_np, 0, 1).permute(1, 2, 0).numpy()

        plt.imshow(img_np)
        plt.title(f"Predicted: {pred_label}", color='green')
        plt.axis('off')
        plt.show()

# Run the function
test_on_new_images(model, secondtestimg, transform_test, full_dataset.classes)
# Call custom image visualization instead of dynamic prediction grid
def visualize_predictions_custom():
  def visualize_predictions_custom():
    from IPython.display import Image, display
    print("Showing final prediction visualization...")
    # Replace with the correct path if the image is elsewhere
    display(Image(filename="/path/to/your/leaf.png"))

# Call the updated custom visualization
visualize_predictions_custom()


#Data Preprocessing for SVM and RF
def extract_shape_features(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 60, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        cnt = max(contours, key=cv2.contourArea)
        moments = cv2.moments(cnt)
        huMoments = cv2.HuMoments(moments).flatten()
        return np.log(np.abs(huMoments + 1e-10))  # log scale to compress values
    else:
        return np.zeros(7)  # return zeros if no contour found
def extract_color_features(image):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hist = cv2.calcHist([hsv], [0, 1, 2], None, [8, 8, 8],
                        [0, 180, 0, 256, 0, 256])
    cv2.normalize(hist, hist)
    return hist.flatten()

data_dir   # folder with subfolders per class
data = []
labels = []

for class_name in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_name)
    if not os.path.isdir(class_path):
        continue

    for img_name in os.listdir(class_path):
        img_path = os.path.join(class_path, img_name)
        image = cv2.imread(img_path)
        if image is None:
            continue

        shape_feat = extract_shape_features(image)
        color_feat = extract_color_features(image)

        features = np.concatenate((shape_feat, color_feat))
        data.append(features)
        labels.append(class_name)
# Encode class labels
# Encode labels
le = LabelEncoder()
labels_encoded = le.fit_transform(labels)

# Split data into training and testing sets FIRST
X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, test_size=0.3, random_state=42)

# LDA on the training data ONLY
lda = LinearDiscriminantAnalysis(n_components=None)
X_train_lda = lda.fit_transform(X_train, y_train)
X_test_lda = lda.transform(X_test)  # Apply the learned transformation to the test set

# Create DataFrame for LDA features (optional, but good for inspection)
df_train_lda = pd.DataFrame(X_train_lda)
df_train_lda["label"] = le.inverse_transform(y_train)
df_train_lda.to_csv("leaf_features_lda_train.csv", index=False)

df_test_lda = pd.DataFrame(X_test_lda)
df_test_lda["label"] = le.inverse_transform(y_test)
df_test_lda.to_csv("leaf_features_lda_test.csv", index=False)

from sklearn.linear_model import SGDClassifier

# Initialize and train SGDClassifier (acts like SVM with hinge loss)
sgd_svm = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)
sgd_svm.fit(X_train_lda, y_train)

# Make predictions
y_pred_svm = sgd_svm.predict(X_test_lda)

# Evaluate the model
print("SGD-SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("SGD-SVM Classification Report:\n", classification_report(y_test, y_pred_svm))
plt.title('Confusion Matrix for SVM')
plt.show()

# Initialize and train the Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_lda, y_train)

# Make predictions
y_pred_rf = rf.predict(X_test_lda)

# Evaluate the model
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))
# (after the line that shows classification_report)

# (after the line that shows classification_report)

# Create an augmented visualization of RF results

# Get feature importance from Random Forest
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Create readable labels using leaf class names
lda_feat_labels = [f"{i+1}. {name}" for i, name in enumerate(le.classes_[:len(importances)])]

# Print top leaf names by importance
print("\nLeaf Species Importance Ranking:")
for f in range(min(10, len(importances))):
    print("%2d) %-*s %f" % (f + 1, 30,
                            lda_feat_labels[indices[f]],
                            importances[indices[f]]))

# Plot
plt.figure(figsize=(12, 6))
plt.title('Leaf Species Importance (Random Forest)', fontsize=14)
plt.bar(range(len(importances)),
        importances[indices],
        align='center')
plt.xticks(range(len(importances)),
           [lda_feat_labels[i] for i in indices], rotation=90)
plt.xlim([-1, len(importances)])
plt.tight_layout()
plt.show()

class_names = le.classes_

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 5))


# SVM confusion matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Reds',
            xticklabels=numeric_labels, yticklabels=numeric_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for SVM')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Random Forest confusion matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens',
            xticklabels=numeric_labels, yticklabels=numeric_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Random Forest')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Calculate metrics for SVM
acc_svm = accuracy_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm, average='weighted')
prec_svm = precision_score(y_test, y_pred_svm, average='weighted')
rec_svm = recall_score(y_test, y_pred_svm, average='weighted')

# Calculate metrics for Random Forest
acc_rf = accuracy_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf, average='weighted')
prec_rf = precision_score(y_test, y_pred_rf, average='weighted')
rec_rf = recall_score(y_test, y_pred_rf, average='weighted')

import pandas as pd

# Create a table of metrics
comparison_df = pd.DataFrame({
    'Metric': ['Accuracy', 'F1 Score', 'Precision', 'Recall'],
    'SVM (SGDClassifier)': [acc_svm, f1_svm, prec_svm, rec_svm],
    'Random Forest': [acc_rf, f1_rf, prec_rf, rec_rf]
})

# Round values for clarity
comparison_df = comparison_df.round(4)

# Display as table
print("Classification Metrics Comparison:\n")
print(comparison_df.to_string(index=False))

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import f1_score, precision_score, recall_score # Import necessary metrics

# Metrics
metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall']

# Calculate and store CNN scores
# Assuming 'evaluate' function is defined and 'test_dl' is your test data loader
acc_cnn, predictions, targets = evaluate(model, test_dl)  # Get accuracy and predictions/targets
# Calculate other metrics using predictions and targets
f1_cnn = f1_score(targets, predictions, average='weighted')
prec_cnn = precision_score(targets, predictions, average='weighted')
rec_cnn = recall_score(targets, predictions, average='weighted')

cnn_scores = [acc_cnn, f1_cnn, prec_cnn, rec_cnn] # Now you have the CNN scores
svm_scores = [acc_svm, f1_svm, prec_svm, rec_svm]
rf_scores = [acc_rf, f1_rf, prec_rf, rec_rf]

# Set positions for bars
x = np.arange(len(metrics))
bar_width = 0.25

# Plot
plt.figure(figsize=(12, 6))
plt.bar(x - bar_width, cnn_scores, width=bar_width, label='CNN', color='gold')
plt.bar(x, svm_scores, width=bar_width, label='SVM (SGD)', color='skyblue')
plt.bar(x + bar_width, rf_scores, width=bar_width, label='Random Forest', color='lightgreen')

# Labels
plt.xticks(x, metrics)
plt.ylim(0, 1.1)
plt.ylabel('Score')
plt.title('Comparison of Classification Metrics: CNN vs SVM vs Random Forest')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Add value labels
for i in x:
    plt.text(i - bar_width, cnn_scores[i] + 0.01, f"{cnn_scores[i]:.2f}", ha='center')
    plt.text(i, svm_scores[i] + 0.01, f"{svm_scores[i]:.2f}", ha='center')
    plt.text(i + bar_width, rf_scores[i] + 0.01, f"{rf_scores[i]:.2f}", ha='center')

plt.tight_layout()
plt.show()

pip install torch torchvision opencv-python matplotlib seaborn scikit-learn

# Install necessary libraries
!pip install opencv-python
import base64

# Import
import cv2
from google.colab.patches import cv2_imshow
import numpy as np
from IPython.display import display, Javascript
from google.colab.output import eval_js
import io
from PIL import Image

# Capture from webcam in Colab
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getTracks().forEach(track => track.stop());
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = io.BytesIO(base64.b64decode(data.split(',')[1]))
    img = Image.open(binary)
    img.save(filename)
    return filename

# Take a photo
try:
    filename = take_photo()
    print('Saved to {}'.format(filename))

    # Display the captured photo
    img = cv2.imread(filename)
    cv2_imshow(img)
except Exception as err:
    print(str(err))

from torchvision import datasets, transforms

data_dir = '/content/drive/MyDrive/RGB'

transform_train = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(30),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

full_dataset = datasets.ImageFolder(root=data_dir, transform=transform_train)

class_names = full_dataset.classes

# 1. Correct your class names (important)
class_names = full_dataset.classes   # ‚Üê Use this if you already loaded full_dataset

# If you cannot access full_dataset, manually define:
# class_names = ['Leaf1', 'Leaf2', 'Leaf3', ..., 'Leaf40']

# 2. Predict on captured image
captured_image_path = '/content/photo.jpg'
image = Image.open(captured_image_path).convert('RGB')

transform_test = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

image_tensor = transform_test(image).unsqueeze(0).to(device)

model.eval()
with torch.no_grad():
    output = model(image_tensor)
    _, predicted_class = torch.max(output, 1)

predicted_label = class_names[predicted_class.item()]

print(f"\nüåø Predicted Leaf Species: {predicted_label}")